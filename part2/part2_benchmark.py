import os
import h5py
import time
import faiss
import requests
import numpy as np
import matplotlib.pyplot as plt
import csv
from typing import Tuple, Dict, List


# æ•°æ®é›†URLå’Œåç§°é…ç½®
DATASETS = [
    {
        'name': 'coco-t2i',
        'url': 'https://github.com/fabiocarrara/str-encoders/releases/download/v0.1.3/coco-t2i-512-angular.hdf5',
        'metric': 'angular'
    },
    {
        'name': 'coco-i2i',
        'url': 'https://github.com/fabiocarrara/str-encoders/releases/download/v0.1.3/coco-i2i-512-angular.hdf5',
        'metric': 'angular'
    },
    {
        'name': 'lastfm',
        'url': 'http://ann-benchmarks.com/lastfm-64-dot.hdf5',
        'metric': 'angular'
    },
    {
        'name': 'mnist',
        'url': 'http://ann-benchmarks.com/mnist-784-euclidean.hdf5',
        'metric': 'euclidean'
    }
]

M_VALUES = [4, 8, 12, 24, 48]
EF_CONSTRUCTION = 200
EF_SEARCH = 200


def download_dataset(url: str, filename: str):
    """ä¸‹è½½æ•°æ®é›†æ–‡ä»¶ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰"""
    filepath = os.path.join('.', filename)
    
    if os.path.exists(filepath):
        print(f"âœ… æ–‡ä»¶å·²å­˜åœ¨: {filepath}")
        return filepath
    
    print(f"ğŸ“¥ ä¸‹è½½ {filename} ...")
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, stream=True, timeout=300)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"\rä¸‹è½½è¿›åº¦: {percent:.1f}% ({downloaded}/{total_size} bytes)", end='', flush=True)
        
        print(f"\nâœ… ä¸‹è½½å®Œæˆ: {filepath}")
        return filepath
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        print(f"è¯·æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†å¹¶ä¿å­˜ä¸º {filepath}")
        print(f"ä¸‹è½½åœ°å€: {url}")
        raise


def load_data_from_url(name: str, url: str, metric: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, str]:
    """
    ä»URLåŠ è½½æ•°æ®é›†
    è¿”å›: (train, test, neighbors, metric)
    """
    filename = url.split("/")[-1]
    filepath = download_dataset(url, filename)
    
    with h5py.File(filepath, 'r') as f:
        print(f"ğŸ“‚ æ•°æ®é›† {name} çš„é”®: {list(f.keys())}")
        train = f['train'][:].astype(np.float32)
        test = f['test'][:].astype(np.float32)
        neighbors = f['neighbors'][:]  # Ground truth æœ€è¿‘é‚»
    
    print(f"  è®­ç»ƒé›†: {train.shape}, æµ‹è¯•é›†: {test.shape}, Ground truth: {neighbors.shape}")
    return train, test, neighbors, metric


def build_hnsw_index(train_data: np.ndarray, M: int, ef_construction: int, metric: str) -> Tuple[faiss.Index, float]:
    """
    æ„å»º HNSW ç´¢å¼•
    è¿”å›: (index, build_time)
    """
    d = train_data.shape[1]
    
    # æ ¹æ®è·ç¦»ç±»å‹é€‰æ‹©åˆé€‚çš„ç´¢å¼•
    if metric == 'angular':
        # Angular è·ç¦»ä½¿ç”¨å†…ç§¯ï¼ˆéœ€è¦å½’ä¸€åŒ–ï¼‰
        train_data = train_data.copy()
        faiss.normalize_L2(train_data)
        index = faiss.IndexHNSWFlat(d, M, faiss.METRIC_INNER_PRODUCT)
    elif metric == 'euclidean':
        # æ¬§æ°è·ç¦»ä½¿ç”¨ L2
        index = faiss.IndexHNSWFlat(d, M, faiss.METRIC_L2)
    else:
        # é»˜è®¤ä½¿ç”¨ L2
        index = faiss.IndexHNSWFlat(d, M, faiss.METRIC_L2)
    
    index.hnsw.efConstruction = ef_construction
    
    # æ„å»ºç´¢å¼•
    start = time.time()
    index.add(train_data)
    build_time = time.time() - start
    
    return index, build_time


def evaluate(index: faiss.Index, test_data: np.ndarray, ground_truth: np.ndarray, 
             ef_search: int, metric: str) -> Tuple[float, float]:
    """
    è¯„ä¼°ç´¢å¼•æ€§èƒ½
    è¿”å›: (recall, qps)
    """
    index.hnsw.efSearch = ef_search
    
    # å¯¹äº angular è·ç¦»ï¼Œéœ€è¦å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
    if metric == 'angular':
        test_data = test_data.copy()
        faiss.normalize_L2(test_data)
    
    # æ‰§è¡ŒæŸ¥è¯¢
    start = time.time()
    D, I = index.search(test_data, 1)
    query_time = time.time() - start
    
    # è®¡ç®— Recall@1
    correct = (I[:, 0] == ground_truth[:, 0]).sum()
    recall = correct / len(test_data)
    
    # è®¡ç®— QPS
    qps = len(test_data) / query_time
    
    return recall, qps


def run_experiment(name: str, url: str, metric: str) -> Dict:
    """
    å¯¹å•ä¸ªæ•°æ®é›†è¿è¡Œå®éªŒ
    è¿”å›ç»“æœå­—å…¸
    """
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ å®éªŒ: {name} (metric={metric})")
    print(f"{'='*80}")
    
    # åŠ è½½æ•°æ®
    train, test, gt, metric = load_data_from_url(name, url, metric)
    
    results = {
        "name": name,
        "M": [],
        "recall": [],
        "qps": [],
        "build_time": [],
        "1_recall": []
    }
    
    # å¯¹æ¯ä¸ª M å€¼è¿›è¡Œæµ‹è¯•
    for M in M_VALUES:
        print(f"\nâ–¶ï¸  æµ‹è¯• M = {M}")
        
        # æ„å»ºç´¢å¼•
        index, build_time = build_hnsw_index(train.copy(), M, EF_CONSTRUCTION, metric)
        print(f"  âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼Œè€—æ—¶: {build_time:.2f}ç§’")
        
        # è¯„ä¼°æ€§èƒ½
        recall, qps = evaluate(index, test.copy(), gt, EF_SEARCH, metric)
        one_minus_recall = 1.0 - recall
        
        print(f"  ğŸ“Š Recall@1: {recall:.4f}")
        print(f"  ğŸ“Š 1-Recall@1: {one_minus_recall:.4f}")
        print(f"  ğŸ“Š QPS: {qps:.2f}")
        
        # è®°å½•ç»“æœ
        results["M"].append(M)
        results["recall"].append(recall)
        results["qps"].append(qps)
        results["build_time"].append(build_time)
        results["1_recall"].append(one_minus_recall)
    
    return results


def plot_metric(results: Dict[str, Dict], xlabel: str, ylabel: str, 
                metric_name: str, y_key: str, output_filename: str):
    """
    ç»˜åˆ¶æŒ‡æ ‡å›¾è¡¨
    """
    plt.figure(figsize=(10, 6))
    
    for name in results:
        x = results[name]["recall"]
        y = results[name][y_key]
        M_list = results[name]["M"]
        
        plt.plot(x, y, marker='o', label=name, linewidth=2, markersize=8)
        
        # ä¸ºæ¯ä¸ªç‚¹æ·»åŠ  M å€¼æ ‡æ³¨
        for i, M in enumerate(M_list):
            plt.text(x[i], y[i], f"  M={M}", fontsize=8, alpha=0.7)
    
    plt.xlabel(xlabel, fontsize=12)
    plt.ylabel(ylabel, fontsize=12)
    plt.title(f"{ylabel} vs {xlabel}", fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_filename, dpi=300)
    print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜: {output_filename}")
    plt.close()


def save_results_to_csv(results: Dict[str, Dict], output_filename: str):
    """
    ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶
    """
    # å‡†å¤‡æ•°æ®è¡Œ
    rows = []
    for name in results:
        for i in range(len(results[name]["M"])):
            rows.append({
                'dataset': name,
                'M': results[name]["M"][i],
                'recall@1': results[name]["recall"][i],
                '1_recall@1': results[name]["1_recall"][i],
                'qps': results[name]["qps"][i],
                'build_time': results[name]["build_time"][i]
            })
    
    # å†™å…¥ CSV
    if rows:
        fieldnames = rows[0].keys()
        with open(output_filename, 'w', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")


def print_summary_table(results: Dict[str, Dict]):
    """
    æ‰“å°ç»“æœæ±‡æ€»è¡¨æ ¼
    """
    print("\n" + "="*80)
    print("ğŸ“‹ ç»“æœæ±‡æ€»")
    print("="*80)
    
    # æ‰“å°è¡¨å¤´
    header = f"{'Dataset':<15} | {'M':>5} | {'Recall@1':>10} | {'1-Recall@1':>12} | {'QPS':>10} | {'Build Time':>12}"
    print(header)
    print("-" * len(header))
    
    # æ‰“å°æ•°æ®
    for name in results:
        for i in range(len(results[name]["M"])):
            row = (f"{name:<15} | "
                   f"{results[name]['M'][i]:>5} | "
                   f"{results[name]['recall'][i]:>10.4f} | "
                   f"{results[name]['1_recall'][i]:>12.4f} | "
                   f"{results[name]['qps'][i]:>10.2f} | "
                   f"{results[name]['build_time'][i]:>12.2f}s")
            print(row)


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸš€ Part 2: HNSW Benchmarking with Increasing Dataset Sizes")
    print("="*80)
    
    # è¿è¡Œæ‰€æœ‰æ•°æ®é›†çš„å®éªŒ
    all_results = {}
    
    for dataset_info in DATASETS:
        try:
            name = dataset_info['name']
            url = dataset_info['url']
            metric = dataset_info['metric']
            
            results = run_experiment(name, url, metric)
            all_results[name] = results
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†æ•°æ®é›† {dataset_info['name']} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
    if all_results:
        print("\n" + "="*80)
        print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        print("="*80)
        
        # å›¾1: QPS vs Recall@1 (ç”¨ä¸åŒæ›²çº¿è¡¨ç¤ºä¸åŒæ•°æ®é›†ï¼Œæ ‡æ³¨Må€¼)
        plot_metric(all_results, "Recall@1", "QPS", "qps", "qps", 
                   "./part2_qps_vs_recall.png")
        
        # å›¾2: Index Build Time vs Recall@1 (ç”¨ä¸åŒæ›²çº¿è¡¨ç¤ºä¸åŒæ•°æ®é›†ï¼Œæ ‡æ³¨Må€¼)
        plot_metric(all_results, "Recall@1", "Index Build Time (s)", 
                   "build_time", "build_time", "./part2_build_time_vs_recall.png")
        
        # ä¿å­˜ CSV ç»“æœ
        save_results_to_csv(all_results, "./part2_results.csv")
        
        # æ‰“å°æ±‡æ€»è¡¨æ ¼
        print_summary_table(all_results)
        
        print("\n" + "="*80)
        print("âœ… æ‰€æœ‰å®éªŒå®Œæˆï¼")
        print("="*80)
    else:
        print("\nâŒ æ²¡æœ‰ç»“æœå¯æ˜¾ç¤ºï¼")


if __name__ == "__main__":
    main()
